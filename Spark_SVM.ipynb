{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-MD75I8FT:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LinearSVC_Pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x221d3d91a20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName('LinearSVC_Pyspark').getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"LinearSVC_Pyspark\") \\\n",
    "    .config(\"spark.executor.memory\", '15G') \\\n",
    "    .config(\"spark.driver.memory\", '50G') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# conf = SparkConf().setAppName(\"LinearSVC_Pyspark\")\n",
    "# conf = (conf.setMaster('local[*]')\n",
    "#         .set('spark.executor.memory', '4G')\n",
    "#         .set('spark.driver.memory', '45G')\n",
    "#         .set('spark.driver.maxResultSize', '10G'))\n",
    "# spark = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50G'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._conf.get('spark.driver.memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import lit\n",
    "from math import sqrt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def calculation(array):\n",
    "    num_items = len(array)\n",
    "    print (num_items, sum(array))\n",
    "    if (num_items !=0):\n",
    "        mean = sum(array) / num_items\n",
    "    else :\n",
    "        mean = 0\n",
    "    differences = [x - mean for x in array]\n",
    "    sq_differences = [d ** 2 for d in differences]\n",
    "    ssd = sum(sq_differences)\n",
    "    if (num_items !=1):\n",
    "        variance = ssd / (num_items - 1)\n",
    "    else :\n",
    "        variance = ssd\n",
    "    sd = sqrt(variance)\n",
    "    return [mean, sd, max(array), min(array)]\n",
    "\n",
    "calcUdf = F.udf(calculation, T.ArrayType(T.FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(values_list):\n",
    "    med = np.median(values_list)\n",
    "    return float(med)\n",
    "udf_median = F.udf(median, T.FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33741500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_watch_acc = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Watch_accelerometer.csv\")\n",
    "df_watch_acc = df_watch_acc.withColumn(\"Sensor\", lit(\"accelerometer\"))\n",
    "df_watch_acc = df_watch_acc.withColumn(\"Device\", lit(\"Watch\"))\n",
    "df_watch_acc=df_watch_acc.select(F.col('User'), F.col('Model'),F.col('Sensor'),F.col('Device'), F.col('gt'), F.col('x').cast('float'), F.col('y').cast('float'), F.col('z').cast('float'))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "df_Phone_acc = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Phones_accelerometer.csv\")\n",
    "df_Phone_acc = df_Phone_acc.withColumn(\"Sensor\", lit(\"accelerometer\"))\n",
    "df_Phone_acc= df_Phone_acc.withColumn(\"Device\", lit(\"Phone\"))\n",
    "df_Phone_acc=df_Phone_acc.select(F.col('User'), F.col('Model'),F.col('Sensor'),F.col('Device'), F.col('gt'), F.col('x').cast('float'), F.col('y').cast('float'), F.col('z').cast('float'))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "df_watch_gyr = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Watch_gyroscope.csv\")\n",
    "df_watch_gyr = df_watch_gyr.withColumn(\"Sensor\", lit(\"gyroscope\"))\n",
    "df_watch_gyr = df_watch_gyr.withColumn(\"Device\", lit(\"Watch\"))\n",
    "df_watch_gyr=df_watch_gyr.select(F.col('User'), F.col('Model'),F.col('Sensor'),F.col('Device'), F.col('gt'), F.col('x').cast('float'), F.col('y').cast('float'), F.col('z').cast('float'))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "df_Phone_gyr = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Phones_gyroscope.csv\")\n",
    "df_Phone_gyr = df_Phone_gyr.withColumn(\"Sensor\", lit(\"gyroscope\"))\n",
    "df_Phone_gyr = df_Phone_gyr.withColumn(\"Device\", lit(\"Phone\"))\n",
    "df_Phone_gyr=df_Phone_gyr.select(F.col('User'), F.col('Model'),F.col('Sensor'),F.col('Device'), F.col('gt'), F.col('x').cast('float'), F.col('y').cast('float'), F.col('z').cast('float'))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "df = df_watch_acc\n",
    "df = df.union(df_Phone_acc)\n",
    "df = df.union(df_watch_gyr)\n",
    "df = df.union(df_Phone_gyr)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+------+-----+-----------+----------+-----------+\n",
      "|User|Model|       Sensor|Device|   gt|          x|         y|          z|\n",
      "+----+-----+-------------+------+-----+-----------+----------+-----------+\n",
      "|   a| gear|accelerometer| Watch|stand| -0.5650316| -9.572019|-0.61411273|\n",
      "|   a| gear|accelerometer| Watch|stand|-0.83258367| -9.713276|-0.60693014|\n",
      "|   a| gear|accelerometer| Watch|stand| -1.0181342| -9.935339|-0.54408234|\n",
      "|   a| gear|accelerometer| Watch|stand| -1.2228385|-10.142437| -0.5662287|\n",
      "|   a| gear|accelerometer| Watch|stand| -1.5771804|-10.480618|-0.40282443|\n",
      "|   a| gear|accelerometer| Watch|stand| -2.1643584|-10.920552|-0.18375498|\n",
      "|   a| gear|accelerometer| Watch|stand|     -2.973|-11.063007| 0.21188685|\n",
      "|   a| gear|accelerometer| Watch|stand| -3.8881836| -11.08276|  0.6847417|\n",
      "|   a| gear|accelerometer| Watch|stand| -4.8919525|-10.890625|    1.01574|\n",
      "|   a| gear|accelerometer| Watch|stand| -12.600683| -7.674015| -1.1791444|\n",
      "|   a| gear|accelerometer| Watch|stand|  -9.214086|-4.5567646|  0.2172738|\n",
      "|   a| gear|accelerometer| Watch|stand|  -9.214086|-4.5567646|  0.2172738|\n",
      "|   a| gear|accelerometer| Watch|stand|  -9.240421| -4.104859| 0.22325931|\n",
      "|   a| gear|accelerometer| Watch|stand|  -9.273342|-3.7295678| 0.24061728|\n",
      "|   a| gear|accelerometer| Watch|stand|    -9.1668|-3.6703112|  -0.729633|\n",
      "|   a| gear|accelerometer| Watch|stand|  -9.153033|-3.6056678| -0.7326257|\n",
      "|   a| gear|accelerometer| Watch|stand| -9.1512375|-3.6122518|-0.74519527|\n",
      "|   a| gear|accelerometer| Watch|stand|  -9.202713|-3.6463692|  -0.729633|\n",
      "|   a| gear|accelerometer| Watch|stand|  -9.288904|-3.7361517| -0.7266402|\n",
      "|   a| gear|accelerometer| Watch|stand|    -9.2907|-3.7720647| -0.7302315|\n",
      "+----+-----+-------------+------+-----+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group BY  (User, Model, gt,Sensor,Device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.groupBy('User', 'Model','gt','Sensor','Device')\\\n",
    "    .agg(calcUdf(F.collect_list(F.col('x'))).alias('x'), calcUdf(F.collect_list(F.col('y'))).alias('y'), calcUdf(F.collect_list(F.col('z'))).alias('z') , udf_median(F.collect_list(F.col('x'))).alias('medianx'),udf_median(F.collect_list(F.col('y'))).alias('mediany'),udf_median(F.collect_list(F.col('z'))).alias('medianz'))\\\n",
    "    .select(F.col('User'), F.col('Model'),F.col('Device'),F.col('Sensor'), F.col('gt'), F.col('x')[0].alias('meanx'), F.col('y')[0].alias('meany'), F.col('z')[0].alias('meanz'), F.col('x')[1].alias('deviationx'), F.col('y')[1].alias('deviationy'), F.col('z')[1].alias('deviationz'), F.col('x')[2].alias('maxx'), F.col('y')[2].alias('maxy'), F.col('z')[2].alias('maxz'), F.col('x')[3].alias('minx'), F.col('y')[3].alias('miny'), F.col('z')[3].alias('minz'),F.col('medianx'),F.col('mediany'),F.col('medianz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"gt\", outputCol=\"label\")\n",
    "df3 = indexer.fit(df2).transform(df2)\n",
    "indexer2 = StringIndexer(inputCol=\"Model\", outputCol=\"ModelIndex\")\n",
    "df4 = indexer2.fit(df3).transform(df3)\n",
    "indexer3 = StringIndexer(inputCol=\"Sensor\", outputCol=\"SensorIndex\")\n",
    "df5 = indexer3.fit(df4).transform(df4)\n",
    "indexer4 = StringIndexer(inputCol=\"Device\", outputCol=\"DeviceIndex\")\n",
    "df6 = indexer4.fit(df5).transform(df5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = df6.select('label', \"meanx\", \"meany\", 'meanz', 'deviationx', 'deviationy', 'deviationz', 'maxx', 'maxy', 'maxz','minx','miny','minz','medianx','mediany','medianz','ModelIndex','SensorIndex','DeviceIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- meanx: float (nullable = true)\n",
      " |-- meany: float (nullable = true)\n",
      " |-- meanz: float (nullable = true)\n",
      " |-- deviationx: float (nullable = true)\n",
      " |-- deviationy: float (nullable = true)\n",
      " |-- deviationz: float (nullable = true)\n",
      " |-- maxx: float (nullable = true)\n",
      " |-- maxy: float (nullable = true)\n",
      " |-- maxz: float (nullable = true)\n",
      " |-- minx: float (nullable = true)\n",
      " |-- miny: float (nullable = true)\n",
      " |-- minz: float (nullable = true)\n",
      " |-- medianx: float (nullable = true)\n",
      " |-- mediany: float (nullable = true)\n",
      " |-- medianz: float (nullable = true)\n",
      " |-- ModelIndex: double (nullable = false)\n",
      " |-- SensorIndex: double (nullable = false)\n",
      " |-- DeviceIndex: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+------+----------+----------+----------+-------+------+-------+--------+--------+-------+-------+-------+-------+----------+-----------+-----------+\n",
      "|label|  meanx|  meany| meanz|deviationx|deviationy|deviationz|   maxx|  maxy|   maxz|    minx|    miny|   minz|medianx|mediany|medianz|ModelIndex|SensorIndex|DeviceIndex|\n",
      "+-----+-------+-------+------+----------+----------+----------+-------+------+-------+--------+--------+-------+-------+-------+-------+----------+-----------+-----------+\n",
      "|  0.0| -7.392|-0.8664|6.0925|     1.417|    1.4106|    1.0939| 0.1203|7.4945|13.5805|-19.6133|-10.5842|-2.9239|-7.3897| -0.978| 6.1537|       4.0|        0.0|        1.0|\n",
      "|  3.0|-9.5261| -3.127|0.1902|    5.6505|    2.1909|    2.5229|16.2967|7.0204|11.9997|-19.6133|-10.8625|-8.1588|-10.413|-3.0658|  0.097|       4.0|        0.0|        1.0|\n",
      "|  0.0|-4.4658| 0.6265|8.4112|    1.1281|    1.4526|    0.6795|-0.4693|6.3973|13.1873| -8.3414| -3.3902| 4.7118|-4.6831|  0.182|  8.418|       1.0|        0.0|        0.0|\n",
      "|  4.0|-0.7464|-0.1177|9.6192|    0.1939|    0.1945|    0.1334|-0.1053|1.3982|10.5058| -1.8004| -1.5993| 8.8585|-0.7374|-0.0958| 9.5864|       1.0|        0.0|        0.0|\n",
      "|  5.0| 0.0113| 0.0013|0.0016|    0.7884|    0.5513|    1.3686| 5.6757|4.7395| 5.0708| -4.3421| -2.9723|-5.2312|-0.0056| 0.0061|-0.0174|       3.0|        1.0|        1.0|\n",
      "+-----+-------+-------+------+----------+----------+----------+-------+------+-------+--------+--------+-------+-------+-------+-------+----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputData = inputData.withColumn('meanx',F.round('meanx',4))\n",
    "inputData = inputData.withColumn('meany',F.round('meany',4))\n",
    "inputData = inputData.withColumn('meanz',F.round('meanz',4))\n",
    "inputData = inputData.withColumn('deviationx',F.round('deviationx',4))\n",
    "inputData = inputData.withColumn('deviationy',F.round('deviationy',4))\n",
    "inputData = inputData.withColumn('deviationz',F.round('deviationz',4))\n",
    "inputData = inputData.withColumn('maxx',F.round('maxx',4))\n",
    "inputData = inputData.withColumn('maxy',F.round('maxy',4))\n",
    "inputData = inputData.withColumn('maxz',F.round('maxz',4))\n",
    "inputData = inputData.withColumn('minx',F.round('minx',4))\n",
    "inputData = inputData.withColumn('miny',F.round('miny',4))\n",
    "inputData = inputData.withColumn('minz',F.round('minz',4))\n",
    "inputData = inputData.withColumn('medianx',F.round('medianx',4))\n",
    "inputData = inputData.withColumn('mediany',F.round('mediany',4))\n",
    "inputData = inputData.withColumn('medianz',F.round('medianz',4))\n",
    "\n",
    "inputData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputData = inputData.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libsvm Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = inputData.rdd\n",
    "d = c.map(lambda line: LabeledPoint(line[0],line[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MLUtils.saveAsLibSVMFile(d,\"libsvm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.textFile(\"libsvm\" + \"/part-*\").coalesce(1).saveAsTextFile(\"libsvm/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedDataDF=spark.read.format(\"libsvm\").load(\"libsvm/data/part-00000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "(train, test) = loadedDataDF.randomSplit([0.67, 0.33])\n",
    "lsvc = LinearSVC(maxIter=15, regParam=0.01)\n",
    "ovr = OneVsRest(classifier=lsvc)\n",
    "ovrModel = ovr.fit(train)\n",
    "predictions = ovrModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.582734\n",
      "Accuracy = 0.417266\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       4.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       4.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       6.0|\n",
      "|  0.0|       6.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       3.0|\n",
      "|  1.0|       3.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label','prediction').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
